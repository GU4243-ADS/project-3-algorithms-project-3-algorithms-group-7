cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
setwd("/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
# Replace the above with your own path or manually set it in RStudio to where this rmd file is located.
experiment_dir <- "/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- FALSE # run cross-validation on the training set
K                 <- 0    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "dog")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir, "test", data_name = "pet", export = TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
tm_feature_test <- NA
if(run.feature.test){
tm_feature_test <- system.time(dat_test <- feature(img_test_dir, "test", data_name = "pet", export = TRUE))
}
#save(dat_train, file = "../output/feature_train.RData")
#save(dat_test, file = "../output/feature_test.RData")
source("../lib/train.R")
source("../lib/test.R")
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file = "../output/fit_train.RData")
tm_test <- NA
if(run.test){
load(file = paste0("../output/feature_", "pet", "_", "test", ".RData"))
load(file = "../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file = "../output/pred_test.RData")
}
label_test <- read.table(paste(experiment_dir, "test_label.txt", sep = ""), header = F)
label_test <- as.numeric(unlist(label_test) == "dog")
library(caret)
confusionMatrix(get(load('/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/output/pred_test.RData')), label_test)$overall['Accuracy']
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for making prediction=", tm_test[1], "s \n")
setwd("/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
# Replace the above with your own path or manually set it in RStudio to where this rmd file is located.
experiment_dir <- "/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
source("../lib/train.R")
source("../lib/test.R")
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
par_best <- list(depth = model_best)
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file = "../output/fit_train.RData")
tm_test <- NA
if(run.test){
load(file = paste0("../output/feature_", "pet", "_", "test", ".RData"))
load(file = "../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file = "../output/pred_test.RData")
}
label_test <- read.table(paste(experiment_dir, "test_label.txt", sep = ""), header = F)
label_test <- as.numeric(unlist(label_test) == "dog")
library(caret)
confusionMatrix(get(load('/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/output/pred_test.RData')), label_test)$overall['Accuracy']
source("../lib/train.R")
source("../lib/test.R")
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
par_best <- list(depth = model_best)
source("../lib/train.R")
source("../lib/test.R")
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
save(fit_train, file = "../output/fit_train.RData")
tm_test <- NA
if(run.test){
load(file = paste0("../output/feature_", "pet", "_", "test", ".RData"))
load(file = "../output/fit_train.RData")
tm_test <- system.time(pred_test <- test(fit_train, dat_test))
save(pred_test, file = "../output/pred_test.RData")
}
label_test <- read.table(paste(experiment_dir, "test_label.txt", sep = ""), header = F)
label_test <- as.numeric(unlist(label_test) == "dog")
library(caret)
confusionMatrix(get(load('/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/output/pred_test.RData')), label_test)$overall['Accuracy']
?kmeans
train_features_dir <- paste(experiment_dir, "train-features/", sep = "")
load(paste(train_features_dir, 'pet', 1, '.jpg.sift.Rdata', sep = ''))
k_clusters = 40
set.seed(1234)
bof <- kmeans(features, k_clusters, iter.max = 20)
for (k in 1 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[k, ])))}
dim(dat1)
dat1 <- matrix(NA, nrow = 1, ncol = 128 * k_clusters)
for (k in 2 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[1, ])), normalize.vector(t(bof$centers[k, ])))}
dat1
k_clusters
t(bof$centers[1, ]))
t(bof$centers[1, ])
i1c1 <- t(bof$centers[1, ])
i1c1 <- normalize.vector(t(bof$centers[1, ]))
for (k in 2 : k_clusters){
dat1 <- cbind(i1c1, normalize.vector(t(bof$centers[k, ])))}
dat1
for (k in 1 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[K, ])), normalize.vector(t(bof$centers[k + k, ])))}
for (k in 1 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[k, ])), normalize.vector(t(bof$centers[k + k, ])))}
for (k in 1 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[k, ])), normalize.vector(t(bof$centers[1 + k, ])))}
for (k in 1 : k_clusters){
dat1 <- cbind(normalize.vector(t(bof$centers[1, ])), normalize.vector(t(bof$centers[k, ])))}
dat1
setwd("/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
# Replace the above with your own path or manually set it in RStudio to where this rmd file is located.
experiment_dir <- "/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
run.cv            <- FALSE # run cross-validation on the training set
K                 <- 0    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- TRUE # process features for test set
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "dog")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
train_features_dir <- paste(experiment_dir, "train-features/", sep = "")
load(paste(train_features_dir, 'pet', 1, '.jpg.sift.Rdata', sep = ''))
k_clusters = 80
set.seed(1234)
bof <- kmeans(features, k_clusters, iter.max = 20)
dat1 <- cbind(normalize.vector(t(bof$centers[1, ])),
normalize.vector(t(bof$centers[2, ])),
normalize.vector(t(bof$centers[3, ])),
normalize.vector(t(bof$centers[4, ])),
normalize.vector(t(bof$centers[5, ])),
normalize.vector(t(bof$centers[6, ])),
normalize.vector(t(bof$centers[7, ])),
normalize.vector(t(bof$centers[8, ])),
normalize.vector(t(bof$centers[9, ])),
normalize.vector(t(bof$centers[10, ])),
normalize.vector(t(bof$centers[11, ])),
normalize.vector(t(bof$centers[12, ])),
normalize.vector(t(bof$centers[13, ])),
normalize.vector(t(bof$centers[14, ])),
normalize.vector(t(bof$centers[15, ])),
normalize.vector(t(bof$centers[16, ])),
normalize.vector(t(bof$centers[17, ])),
normalize.vector(t(bof$centers[18, ])),
normalize.vector(t(bof$centers[19, ])),
normalize.vector(t(bof$centers[20, ])),
normalize.vector(t(bof$centers[21, ])),
normalize.vector(t(bof$centers[22, ])),
normalize.vector(t(bof$centers[23, ])),
normalize.vector(t(bof$centers[24, ])),
normalize.vector(t(bof$centers[25, ])),
normalize.vector(t(bof$centers[26, ])),
normalize.vector(t(bof$centers[27, ])),
normalize.vector(t(bof$centers[28, ])),
normalize.vector(t(bof$centers[29, ])),
normalize.vector(t(bof$centers[30, ])),
normalize.vector(t(bof$centers[31, ])),
normalize.vector(t(bof$centers[32, ])),
normalize.vector(t(bof$centers[33, ])),
normalize.vector(t(bof$centers[34, ])),
normalize.vector(t(bof$centers[35, ])),
normalize.vector(t(bof$centers[36, ])),
normalize.vector(t(bof$centers[37, ])),
normalize.vector(t(bof$centers[38, ])),
normalize.vector(t(bof$centers[39, ])),
normalize.vector(t(bof$centers[40, ])),
normalize.vector(t(bof$centers[41, ])),
normalize.vector(t(bof$centers[42, ])),
normalize.vector(t(bof$centers[43, ])),
normalize.vector(t(bof$centers[44, ])),
normalize.vector(t(bof$centers[45, ])),
normalize.vector(t(bof$centers[46, ])),
normalize.vector(t(bof$centers[47, ])),
normalize.vector(t(bof$centers[48, ])),
normalize.vector(t(bof$centers[49, ])),
normalize.vector(t(bof$centers[50, ])),
normalize.vector(t(bof$centers[51, ])),
normalize.vector(t(bof$centers[52, ])),
normalize.vector(t(bof$centers[53, ])),
normalize.vector(t(bof$centers[54, ])),
normalize.vector(t(bof$centers[55, ])),
normalize.vector(t(bof$centers[56, ])),
normalize.vector(t(bof$centers[57, ])),
normalize.vector(t(bof$centers[58, ])),
normalize.vector(t(bof$centers[59, ])),
normalize.vector(t(bof$centers[60, ])),
normalize.vector(t(bof$centers[61, ])),
normalize.vector(t(bof$centers[62, ])),
normalize.vector(t(bof$centers[63, ])),
normalize.vector(t(bof$centers[64, ])),
normalize.vector(t(bof$centers[65, ])),
normalize.vector(t(bof$centers[66, ])),
normalize.vector(t(bof$centers[67, ])),
normalize.vector(t(bof$centers[68, ])),
normalize.vector(t(bof$centers[69, ])),
normalize.vector(t(bof$centers[70, ])),
normalize.vector(t(bof$centers[71, ])),
normalize.vector(t(bof$centers[72, ])),
normalize.vector(t(bof$centers[73, ])),
normalize.vector(t(bof$centers[74, ])),
normalize.vector(t(bof$centers[75, ])),
normalize.vector(t(bof$centers[76, ])),
normalize.vector(t(bof$centers[77, ])),
normalize.vector(t(bof$centers[78, ])),
normalize.vector(t(bof$centers[79, ])),
normalize.vector(t(bof$centers[80, ]))
)
dat1
dim(dat1)
128*80
for (i in 2 : 4) {
load(paste(train_features_dir, 'pet', i, '.jpg.sift.Rdata', sep = ''))
set.seed(1234)
bof <- kmeans(features, k_clusters, iter.max = 20)
dat1 <- rbind(dat1, cbind(normalize.vector(t(bof$centers[1, ])),
normalize.vector(t(bof$centers[2, ])),
normalize.vector(t(bof$centers[3, ])),
normalize.vector(t(bof$centers[4, ])),
normalize.vector(t(bof$centers[5, ])),
normalize.vector(t(bof$centers[6, ])),
normalize.vector(t(bof$centers[7, ])),
normalize.vector(t(bof$centers[8, ])),
normalize.vector(t(bof$centers[9, ])),
normalize.vector(t(bof$centers[10, ])),
normalize.vector(t(bof$centers[11, ])),
normalize.vector(t(bof$centers[12, ])),
normalize.vector(t(bof$centers[13, ])),
normalize.vector(t(bof$centers[14, ])),
normalize.vector(t(bof$centers[15, ])),
normalize.vector(t(bof$centers[16, ])),
normalize.vector(t(bof$centers[17, ])),
normalize.vector(t(bof$centers[18, ])),
normalize.vector(t(bof$centers[19, ])),
normalize.vector(t(bof$centers[20, ])),
normalize.vector(t(bof$centers[21, ])),
normalize.vector(t(bof$centers[22, ])),
normalize.vector(t(bof$centers[23, ])),
normalize.vector(t(bof$centers[24, ])),
normalize.vector(t(bof$centers[25, ])),
normalize.vector(t(bof$centers[26, ])),
normalize.vector(t(bof$centers[27, ])),
normalize.vector(t(bof$centers[28, ])),
normalize.vector(t(bof$centers[29, ])),
normalize.vector(t(bof$centers[30, ])),
normalize.vector(t(bof$centers[31, ])),
normalize.vector(t(bof$centers[32, ])),
normalize.vector(t(bof$centers[33, ])),
normalize.vector(t(bof$centers[34, ])),
normalize.vector(t(bof$centers[35, ])),
normalize.vector(t(bof$centers[36, ])),
normalize.vector(t(bof$centers[37, ])),
normalize.vector(t(bof$centers[38, ])),
normalize.vector(t(bof$centers[39, ])),
normalize.vector(t(bof$centers[40, ])),
normalize.vector(t(bof$centers[41, ])),
normalize.vector(t(bof$centers[42, ])),
normalize.vector(t(bof$centers[43, ])),
normalize.vector(t(bof$centers[44, ])),
normalize.vector(t(bof$centers[45, ])),
normalize.vector(t(bof$centers[46, ])),
normalize.vector(t(bof$centers[47, ])),
normalize.vector(t(bof$centers[48, ])),
normalize.vector(t(bof$centers[49, ])),
normalize.vector(t(bof$centers[50, ])),
normalize.vector(t(bof$centers[51, ])),
normalize.vector(t(bof$centers[52, ])),
normalize.vector(t(bof$centers[53, ])),
normalize.vector(t(bof$centers[54, ])),
normalize.vector(t(bof$centers[55, ])),
normalize.vector(t(bof$centers[56, ])),
normalize.vector(t(bof$centers[57, ])),
normalize.vector(t(bof$centers[58, ])),
normalize.vector(t(bof$centers[59, ])),
normalize.vector(t(bof$centers[60, ])),
normalize.vector(t(bof$centers[61, ])),
normalize.vector(t(bof$centers[62, ])),
normalize.vector(t(bof$centers[63, ])),
normalize.vector(t(bof$centers[64, ])),
normalize.vector(t(bof$centers[65, ])),
normalize.vector(t(bof$centers[66, ])),
normalize.vector(t(bof$centers[67, ])),
normalize.vector(t(bof$centers[68, ])),
normalize.vector(t(bof$centers[69, ])),
normalize.vector(t(bof$centers[70, ])),
normalize.vector(t(bof$centers[71, ])),
normalize.vector(t(bof$centers[72, ])),
normalize.vector(t(bof$centers[73, ])),
normalize.vector(t(bof$centers[74, ])),
normalize.vector(t(bof$centers[75, ])),
normalize.vector(t(bof$centers[76, ])),
normalize.vector(t(bof$centers[77, ])),
normalize.vector(t(bof$centers[78, ])),
normalize.vector(t(bof$centers[79, ])),
normalize.vector(t(bof$centers[80, ]))
))
}
dat1
dim(dat1)
dat2 <- matrix(NA, 4, 1764)
for(i in 1 : 4){
img     <- cv2$imread(paste0(img_train_dir, 'pet', i, ".jpg")) / 255
img_resized <- cv2$resize(img, dsize=tuple(64L, 64L))
hog_values <- hog$compute(np_array(img_resized * 255, dtype='uint8'))
dat2[i, ] <- t(hog_values)
}
library(reticulate)
cv2 <- reticulate::import('cv2')
library("EBImage")
dat2 <- matrix(NA, 4, 1764)
for(i in 1 : 4){
img     <- cv2$imread(paste0(img_train_dir, 'pet', i, ".jpg")) / 255
img_resized <- cv2$resize(img, dsize=tuple(64L, 64L))
hog_values <- hog$compute(np_array(img_resized * 255, dtype='uint8'))
dat2[i, ] <- t(hog_values)
}
winSize <- tuple(64L, 64L)
blockSize <- tuple(16L, 16L)
blockStride <- tuple(8L, 8L)
cellSize <- tuple(8L, 8L)
nbins = 9L
hog = cv2$HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)
dat2 <- matrix(NA, 4, 1764)
for(i in 1 : 4){
img     <- cv2$imread(paste0(img_train_dir, 'pet', i, ".jpg")) / 255
img_resized <- cv2$resize(img, dsize=tuple(64L, 64L))
hog_values <- hog$compute(np_array(img_resized * 255, dtype='uint8'))
dat2[i, ] <- t(hog_values)
}
dat2
dim(dat2)
dat <- cbind(dat1, dat2)
dim(dat)
128*80 + 1764
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
bof <- kmeans(features, k_clusters, iter.max = 60)
k_clusters
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
if(!require("randomForest")){
install.packages("randomForest")
}
if(!require("ppls")){
install.packages("ppls")
}
if(!require("caret")){
install.packages("caret")
}
# if(!require("xgboost")){
#   install.packages("xgboost")
# }
# if(!require("e1071")){
#   install.packages("e1071")
# }
# if(!require("RcppCNPy")){
#   install.packages("RcppCNPy")
# }
library("EBImage")
library("gbm")
library("randomForest")
library("ppls")
library("caret")
# library("xgboost")
# library("e1071")
# library("RcppCNPy")
setwd("/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/doc")
# Replace the above with your own path or manually set it in RStudio to where this rmd file is located.
experiment_dir <- "/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- FALSE # run cross-validation on the training set
K                 <- 0    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "dog")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
k_clusters
version
library(xgboost)
?xgb
?xgboost
if(!require("EBImage")){
source("https://bioconductor.org/biocLite.R")
biocLite("EBImage")
}
if(!require("gbm")){
install.packages("gbm")
}
if(!require("randomForest")){
install.packages("randomForest")
}
if(!require("ppls")){
install.packages("ppls")
}
if(!require("caret")){
install.packages("caret")
}
# if(!require("xgboost")){
#   install.packages("xgboost")
# }
# if(!require("e1071")){
#   install.packages("e1071")
# }
# if(!require("RcppCNPy")){
#   install.packages("RcppCNPy")
# }
library("EBImage")
library("gbm")
library("randomForest")
library("ppls")
library("caret")
# library("xgboost")
# library("e1071")
# library("RcppCNPy")
experiment_dir <- "/Users/qinqingao/Documents/GitHub/project-2-predictive-modelling-group-5/data/" # This will be modified for different data sets.
img_train_dir  <- paste(experiment_dir, "train/", sep="")
img_test_dir   <- paste(experiment_dir, "test/", sep="")
run.cv            <- FALSE # run cross-validation on the training set
K                 <- 5    # number of CV folds
run.feature.train <- TRUE # process features for training set
run.test          <- TRUE # run evaluation on an independent test set
run.feature.test  <- TRUE # process features for test set
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.table(paste(experiment_dir, "train_label.txt", sep = ""), header = F)
label_train <- as.numeric(unlist(label_train) == "dog")
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
tm_feature_train <- system.time(dat_train <- feature(img_train_dir, "train", data_name = "pet", export = TRUE))
}
object.size()
object.size('a')
object.size(1)
a <- read.csv('/Users/qinqingao/Downloads/datafest2018NewApril6.csv')
head(a)
source('~/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/doc/memory_based_model_short.R', echo=TRUE)
save(MS_sim, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_sim.RData")
###########################################################
######## Calculating the Predictions for the Users ########
###########################################################
# Calculate predictions for MS
# This calculation took me 15 minutes
system.time(MS_pred <- pred_matrix(MS_UI, MS_sim))
#object.size(MS_pred <- pred_matrix(MS_UI, MS_sim))
save(MS_pred, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_pred.RData")
# Calculate predictions for movies
# This calculation took me 2493 second
system.time(movie_pred <- pred_matrix(movie_UI, movie_sim))
#object.size(movie_pred <- pred_matrix(movie_UI, movie_sim))
save(movie_pred, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/movie_pred.RData")
