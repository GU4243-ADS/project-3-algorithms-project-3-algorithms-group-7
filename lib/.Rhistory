setwd("~/Documents/GitHub/Spring2018/Project_Starter_Codes/Project2-PredictiveModelling/doc")
knitr::opts_chunk$set(echo = TRUE)
library("EBImage")
library("gbm")
img_train_dir  <- "../../../../Proj2_Data/train/" # This is where my data lives (outside of Spring2018)
n_files <- length(list.files(img_train_dir))
n_files
list.files(img_train_dir)
length(list.files(img_train_dir))
n_files <- length(list.files(img_train_dir))
n_files
dat <- matrix(NA, nrow = n_files, ncol = 3)
for(i in 1:n_files){
img <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i, 1:length(dim(img))] <- dim(img)
}
# How many B/W images?  All color.
table(dat[, 3])
# How many rows? A lot at 500 rows.  Maybe a good subset to consider.
table(dat[, 1])
cv.function <- function(X.train, y.train, d, K) {
n        <- length(y.train)
n.fold   <- floor(n/K)
s        <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
cv.error <- rep(NA, K)
for (i in 1:K){
train.data  <- X.train[s != i,]
train.label <- y.train[s != i]
test.data   <- X.train[s == i,]
test.label  <- y.train[s == i]
par  <- list(depth = d)
fit  <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error[i] <- mean(pred != test.label)
}
return(c(mean(cv.error), sd(cv.error)))
}
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
la
lab
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i,] <- rowMeans(img)
}
i
rowMeans(img)
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[row,] <- rowMeans(img)
row <- row + 1
}
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train
label_train <- label_train[subset]
dim(da)
dim(dat)
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
dat
dim(dat)
n_files <- length(list.files(img_train_dir))
### store image dimensions
dat <- matrix(NA, nrow = n_files, ncol = 3)
for(i in 1:n_files){
img <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[i, 1:length(dim(img))] <- dim(img)
}
# How many B/W images?  All color.
table(dat[, 3])
# How many rows? A lot at 500 rows.  Maybe a good subset to consider.
table(dat[, 1])
n_r    <- 500
subset <- which(dat[,1] == n_r)
### store vectorized pixel values of images
dat <- matrix(NA, length(subset), n_r)
row <- 1
dim(dat)
for(i in subset){
img     <- readImage(paste0(img_train_dir,  "pet", i, ".jpg"))
dat[row,] <- rowMeans(img)
row <- row + 1
}
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train <- label_train[subset]
length(label_train)
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
train <- function(dat_train, label_train, par = NULL){
### Train a Gradient Boosting Model (GBM) using processed features from training images
### Input:
###  -  processed features from images
###  -  class labels for training images
### Output: training model specification
### load libraries
library("gbm")
### Train with gradient boosting model
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x = dat_train, y = label_train,
n.trees = 2000,
distribution = "bernoulli",
interaction.depth = depth,
bag.fraction = 0.5,
verbose = FALSE)
best_iter <- gbm.perf(fit_gbm, method = "OOB", plot.it = FALSE)
return(list(fit = fit_gbm, iter = best_iter))
}
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
source("../lib/cross_validation.R")
source("../lib/train.R")
source("../lib/test.R")
model_values <- seq(3, 11, 2)
model_labels <- paste("GBM with depth =", model_values)
label_train <- read.csv("../../../../Proj2_Data/train_label.txt", header = F)
label_train <- as.numeric(unlist(label_train) == "cat")
label_train <- label_train[subset]
K <- 5
err_cv <- array(dim = c(length(model_values), 2))
for(k in 1:length(model_values)){
cat("k=", k, "\n")
err_cv[k,] <- cv.function(dat, label_train, model_values[k], K)
}
warnings()
plot(model_values, err_cv[,1], xlab = "Interaction Depth", ylab = "CV Error",
main = "Cross Validation Error", type = "n", ylim = c(0, 0.25))
points(model_values, err_cv[,1], col = "blue", pch=16)
lines(model_values, err_cv[,1], col = "blue")
arrows(model_values, err_cv[,1] - err_cv[,2], model_values, err_cv[,1] + err_cv[,2],
length = 0.1, angle = 90, code = 3)
err_cv[,1]
err_cv
model_best <- model_values[1]
if(run.cv){
model_best <- model_values[which.min(err_cv[, 1])]
}
par_best <- list(depth = model_best)
model_best <- model_values[which.min(err_cv[, 1])]
par_best <- list(depth = model_best)
par_best
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat_train, label_train, par_best))
tm_train <- NA
tm_train <- system.time(fit_train <- train(dat, label_train, par_best))
dim(get(load('/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample/MS_pred.RData')))
dim(get(load('/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_pred.RData')))
?cosine
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
cosine(rowA, rowB)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample")
movie_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
movie_train <- movie_train[, 2:4] # get rid of first column: row number
users  <- sort(unique(movie_train$User))
movies <- sort(unique(movie_train$Movie))
UI            <- matrix(NA, nrow = length(users), ncol = length(movies))
row.names(UI) <- users
colnames(UI)  <- movies
movies  <- movie_train$Movie[movie_train$User == users[1]]
ratings <- movie_train$Score[movie_train$User == users[1]]
ord     <- order(movies)
movies  <- movies[ord]
ratings <- ratings[ord]
system.time(UI[1, colnames(UI) %in% movies] <- ratings)
movie_UI <- movie_data_transform(movie_train)
source("functions.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions.R")
movie_UI         <- as.matrix(movie_UI)
movie_sim_weight <- matrix(NA, nrow = nrow(movie_UI), ncol = nrow(movie_UI))
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
movie_UI <- movie_data_transform(movie_train)
movie_UI         <- as.matrix(movie_UI)
movie_sim_weight <- matrix(NA, nrow = nrow(movie_UI), ncol = nrow(movie_UI))
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
cosine(rowA, rowB)
user_similar <- diag(dim(movie_sim_weight)[1])
user_similar
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_VS.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_VS.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/MS_sample")
MS_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
MS_train <- MS_train[, 2:4] # get rid of first column: row number
MS_UI <- MS_data_transform(MS_train)
visit_nums <- rowSums(MS_UI != 0)
table(visit_nums)
mean(visit_nums)
median(visit_nums)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample")
movie_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
movie_train <- movie_train[, 2:4] # get rid of first column: row number
movie_UI <- movie_data_transform(movie_train)
movie_sim <- calc_weight(movie_UI)
?entropy
install.packages('entropy')
library(entropy)
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
rowA
rowB
cor(rowA, rowB, method = 'pearson', use = "pairwise.complete.obs")
vec1 <- apply(movie_UI, 1, cor, movie_UI[1, ], method = 'pearson', use = "pairwise.complete.obs")
vec1
library(lsa)
install.packages('lsa')
library(lsa)
packages.used <- c('lsa')
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(lsa)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_vs.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/MS_sample")
MS_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
MS_train <- MS_train[, 2:4] # get rid of first column: row number
MS_UI <- MS_data_transform(MS_train)
save(MS_UI, file = "../output/MS_UI.RData")
save(MS_UI, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_UI.RData")
visit_nums <- rowSums(MS_UI != 0)
table(visit_nums)
mean(visit_nums)
median(visit_nums)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample")
movie_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
movie_train <- movie_train[, 2:4] # get rid of first column: row number
movie_UI <- movie_data_transform(movie_train)
total_ratings <- rowSums(movie_UI, na.rm = TRUE)
table(total_ratings)
mean(total_ratings)
median(total_ratings)
movie_sim <- calc_weight(movie_UI)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_vs.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample")
movie_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
movie_train <- movie_train[, 2:4] # get rid of first column: row number
movie_UI <- movie_data_transform(movie_train)
movie_sim <- calc_weight(movie_UI)
movie_sim <- calc_weight(movie_UI)
movie_sim <- calc_weight(movie_UI)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_vs.R")
movie_sim <- calc_weight(movie_UI)
save(movie_sim, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/movie_sim.RData")
MS_sim <- calc_weight(MS_UI)
save(MS_sim, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_sim.RData")
MS_pred <- pred_matrix(MS_UI, MS_sim)
save(MS_pred, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_pred.RData")
movie_pred <- pred_matrix(movie_UI, movie_sim)
save(movie_pred, file = "/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/movie_pred.RData")
library(entropy)
?Entropy
??Entropy
?entropy
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_entropy.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_entropy.R")
movie_sim <- calc_weight(movie_UI)
movie_sim <- calc_weight(movie_UI)
source("functions_entropy.R")
source("functions_entropy.R")
packages.used <- c('lsa', 'entropy')
packages.used <- c('lsa', 'entropy')
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(lsa)
library(entropy)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_entropy.R")
movie_sim <- calc_weight(movie_UI)
return(entropy(rowA))
return(entropy([rowA, rowB]))
return(entropy(c(rowA, rowB)))
source("functions_entropy.R")
MS_sim <- calc_weight(MS_UI)
install.packages('Entropy')
?MutInf
??MutInf
source("functions_entropy.R")
MS_sim <- calc_weight(MS_UI)
source("functions_entropy.R")
MS_sim <- calc_weight(MS_UI)
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
cor(rowA, rowB, method = 'pearson', use = "pairwise.complete.obs")
entropy(table(rowA, rowB))
entropy(rowA)
rowA
joint_values <- !is.na(rowA) & !is.na(rowB)
joint_values
cor(rowA[joint_values], rowB[joint_values], method = 'pearson')
library(infotheo)
install.packages('infotheo')
library(infotheo)
mutinformation(rowA, rowB)
entropy(rowA)
entropy(rowB)
entropy(table(rowA, rowB))
entropy(rowA) + entropy(rowB) - entropy(table(rowA, rowB))
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
entropy(table(rowA, rowB))
entropy(rowA)
entropy(table(rowA))
entropy(table(rowA)) + entropy(table(rowB)) - entropy(table(rowA, rowB))
entropy(rowA) + entropy(rowB) - entropy(table(rowA, rowB))
entropy(table(rowB))
?mutinformation
source("functions_entropy.R")
MS_sim <- calc_weight(MS_UI)
packages.used <- c('lsa', 'infotheo')
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))
if(length(packages.needed) > 0) {
install.packages(packages.needed, dependencies = TRUE, repos = 'http://cran.us.r-project.org')
}
library(lsa)
library(infotheo)
library(infotheo)
source("functions_entropy.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/MS_sample")
MS_train <- read.csv("data_train.csv", as.is = TRUE, header = TRUE)
MS_train <- MS_train[, 2:4] # get rid of first column: row number
head(MS_UI)
table(visit_nums)
mean(visit_nums)
median(visit_nums)
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/data/eachmovie_sample")
table(total_ratings)
mean(total_ratings)
median(total_ratings)
movie_sim <- calc_weight(movie_UI)
source("functions_msd.R")
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_msd.R")
MS_sim <- calc_weight(MS_UI)
rowA <- movie_UI[1, ]
rowB <- movie_UI[2, ]
(rowA - rowB)^2
diff(rowA, rowB)
(rowA - rowB)
(rowA - rowB)^2
as.vector(rowA)
as.vector(rowA) - as.vector(rowB)
(as.vector(rowA) - as.vector(rowB))^2
rowA <- movie_UI[1, ]
rowA
mean(rowA-rowB)
joint_values <- !is.na(rowA) & !is.na(rowB)
mean(rowA[joint_values] - rowBjoint_values)
mean(rowA[joint_values] - rowB[joint_values])
mean(rowA[joint_values] - rowB[joint_values])^2
(mean(rowA[joint_values] - rowB[joint_values]))^2
library(metrics)
install.packages('metrics')
install.packages('Metrics')
library(Metrics)
mse(rowA[joint_values], rowB[joint_values])
?mse
actual <- c(1.1, 1.9, 3.0, 4.4, 5.0, 5.6)
predicted <- c(0.9, 1.8, 2.5, 4.5, 5.0, 6.2)
mse(actual, predicted)
mean((rowA[mse(actual, predicted)] - rowB[mse(actual, predicted)])^2)
mean((rowA[joint_values] - rowB[joint_values])^2)
mean(rowA[joint_values] - rowB[joint_values])^2
mean(rowA[joint_values] - rowB[joint_values])
mean(rowA[joint_values] - rowB[joint_values])^2
mean(rowA[joint_values] - rowB[joint_values])^2
setwd("/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/lib")
source("functions_msd.R")
movie_sim <- calc_weight(movie_UI)
mean((rowA[joint_values]-rowB[joint_values])^2)
mean(rowA[joint_values] - rowB[joint_values])^2
load('/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/MS_pred_Pearson.RData')
View(MS_pred)
load('/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/movie_pred_vs.RData')
View(movie_pred)
load('/Users/qinqingao/Documents/GitHub/project-3-algorithms-project-3-algorithms-group-7/output/movie_pred_Pearson.RData')
View(movie_pred)
